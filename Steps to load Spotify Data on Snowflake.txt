1. Create two tables on SnowflakeDB - tblAlbum, tblArtist
2. Create Storage Stage to test connection with AWS
3. Test copy command and Load Data
4. Create 2 Snowpipe for individual folder
5. Run complete pipeline to test it 


https://bitbucket.org/ravi_vaddi/dataengineering-project-api-pandas-aws-lambda-glue-athena/src/develop/

 125  git  config --global user,name "ravi_vaddi"
  126  git  config --global user,name "ravi vaddi"
  127  git  config --global user.name "ravi_vaddi"
  128  git config --global user.email "vaddi.ravirao@gmail.com"
  129  git config --global core.editor "vi"
  130  git config --global core.editor "vi"
  131  git config --global core.editor "C:\Program Files\Notepad++/notepad++.exe"
  132  git config --global core.editor "'C:\Program Files\Notepad++/notepad++.exe'"
  133  git config --list
  134  git init
  135  git remote add origin https://bitbucket.org/ravi_vaddi/dataengineering-project-api-pandas-aws-lambda-glue-athena/src/develop/
  136  git remote -v
  137  git branch
  138  git branch --vv
  139  git branch --vv
  140  git branch -show-current
  141  git branch --show-current
  142  git branch -vv
  143  git status
  144  exit
  145  history

DataEngineering Project (API + Pandas + AWS lambda + Glue + Athena + CloudWatch) -->
This project focuses on retrieving semi-structured JSON data from an API and transforming it into a structured format using the Pandas library. The structured data is then processed using AWS Glue to create data catalog tables. Finally, Amazon Athena is employed to query the data catalog tables, providing a relational table-like experience for data analysis. The project is first built locally then we use AWS Lambda to transform similar functionality over cloud.

In this project we have kept the extraction part similar to 

[Repository Name](https://github.com/username/repository-name)

where data is first read from an API and loaded into AWS S3 folder

Transformation is done through AWS Spark i.e through Glue Notebooks specifically - where data is read from AWS S3 which is JSON then converted into Glue Dynamic Dataframe and transformed using PySpark explode function which is written back to AWS S3 in CSV Format

Instead of AWS Glue Crawler and Athena we are using Snowflake with SnowPipe which is configured for auto-ingestion once transformed AWS file lands to S3 bucket . 

Data is later analysed using Snowflake table